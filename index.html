<!doctype html>
<html lang="en">

<head>

    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
    <meta content="Master Student, Peking University" name="description">
    <meta content="Yuxuan Wang" name="author">
    <meta content="#222222" name="theme-color">
    <meta content="Yuxuan Wang, dialogue, llm, reasoning, vision language, multimodal, pku, bigai, homepage" name="keywords">

    <!-- <link href="images/nicons/pku_logo.png" rel="icon" sizes="16x16" type="image/png"> -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="assets/style.css" rel="stylesheet">
    <link href="assets/font-awesome.min.css" rel="stylesheet">

    <title>Yuxuan Wang's Homepage</title>
</head>

<body>
    <!-- <h1>Hello, world!</h1> -->
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr style="padding:0px">
            <td style="padding:0px">

                
                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr style="padding:0px">
                    <td style="padding:2.5%;width:63%;vertical-align:middle">
                        <p class="name"><b>Yuxuan Wang</b></p>
                        <p class="email"><code>flagwyx [at] gmail.com</code></p>

                        I am currently a research engineer at the NLP Lab of the <a href="https://www.bigai.ai/">Beijing Institute for General Artificial Intelligence (BIGAI)</a>, led by <a href="https://zilongzheng.github.io/">Zilong Zheng</a> and <a href="https://zhusongchun.net/">Songchun Zhu</a>. 
                        Prior to this, I obtained my Master's degree from <a href="https://www.pku.edu.cn/" >Peking University</a> under the supervision of <a href="https://www.wict.pku.edu.cn/zhaodongyan/en/">Dongyan Zhao</a>. 
                        Additionally, I completed a summer internship at <a href="https://www.jhu.edu/">Johns Hopkins University</a>, where I was mentored by <a href="https://lizw14.github.io/">Zhuowan Li</a> and <a href="https://www.cs.jhu.edu/~ayuille/">Alan L. Yuille</a>. Currently, I also collaborate with <a href="https://cihangxie.github.io/">Cihang Xie</a>  at the <a href="https://www.ucsc.edu/">University of California, Santa Cruz</a>.  
                        My current research primarily concentrates on the domains of video-language learning and multimodal agents. I am especially captivated by studies that provide novel insights and helpful applications.
                        
                        <br> <br>

                        <!-- <b> I am looking for a PhD position. Please feel free to contact me without any hesitation! </b> -->
                        <b>I am looking for additional computing resources for omni-LM and open-world modeling research. Please feel free to contact me!</b>
                        

                    </td>
                    <td style="padding:15% 7% 7% 7%;width:40%;max-width:40%">
                        <!-- <img class="profilepic pt-2 pb-4" src="https://cdn.jsdelivr.net/gh/patrick-tssn/patrick-tssn.github.io@v1.0.0/images/profile.png"> -->
                        <img class="profilepic pt-2 pb-4" src="images/profile.jpg" style="width:100%;max-width:100%">
                        <!-- <a href="images/profile.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/profile.png" class="hoverZoomLink"></a> -->
                        <div>
                            <a class="menulink" href="https://scholar.google.com/citations?user=jNCX2g0AAAAJ&hl=en"
                                target="_blank"><code>Scholar</code></a> â€¢
                            <a class="menulink" href="https://github.com//patrick-tssn" target="_blank"><code>Github</code></a>  â€¢
                            <!-- <a class="menulink" href="https://twitter.com/Patrick_TSSN" target="_blank">Twitter</a> / -->
                            <a class="menulink" href="https://cdn.jsdelivr.net/gh/patrick-tssn/patrick-tssn.github.io@1be6f23/images/Yuxuan_Wang_CV.pdf" target="_blank"><code>CV</code></a>
                            <!-- <a class="menulink" target="_blank"><code><del>CV</del></code></a> -->
                        </div>
                    </td>
                    </tr>
                </tbody></table>

                <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr>
                        <td style="padding:20px;width:100%;vertical-align:middle">
                            <p class="header pt-3"> <b>News</b> ðŸ”¥ </p>
                            <li> 2023.05: 1 paper got accepted by ACL 2023. <img src="images/new.gif"/> </li>
                            <li> 2023.05: 1 paper got accepted by ACL 2023 Findings. <img src="images/new.gif"/> </li>
                        </td>
                    </tr>
                </tbody></table> -->

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                      <!-- <heading>Publications and Preprints</heading> -->
                      <p class="header pt-4"> <b>Selected Publications</b> <span class="noter" style="color:#7d7d7d"> (* = equal
                        contribution)</span>
                </p>
                    </td>
                  </tr>
                </tbody></table>
                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                
                <tr></tr>
                    <!-- <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one" >
                            <img  class="overviewpic pt-2 pb-4" src="papers/figures/xxx.png"> 
                        </div>
                    </td> -->
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <span class="papertitle">Friends-MMC: Dataset for Multi-modal Multi-party Conversation Understanding</span>
                        <br>
                        Yueqian Wang, Xiaojun Meng, <span class="thisauthor">Yuxuan Wang</span>, Jianxin Liang, Qun Liu, Dongyan Zhao
                        <br>
                        <span class="conf">
                            <a class="confshort" href="https://aaai.org/conference/aaai/aaai-25/"><b> AAAI 2025 </b> </a>
                            <span class="tagsep">|</span>
                            <a class="tag" href="https://arxiv.org/abs/2412.17295" target="_blank"><i
                                    class="fa fa-file-pdf-o" aria-hidden="true"></i> PDF</a>
                            <span class="tagsep">|</span>
                            <a class="tag" href="https://github.com/yellow-binary-tree/Friends-MMC"
                                target="_blank"><i class="fa fa-github-alt" aria-hidden="true"></i> Code </a>
                            <span class="tagsep">|</span>
                            <a class="tag" href="papers/aaai25_friendsmmc.bib" target="_blank"><i class="fa fa-download"
                                    aria-hidden="true"></i> Cite</a>
                        </span>
                    </td>
                </tr>    

                <tr></tr>
                    <!-- <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one" >
                            <img  class="overviewpic pt-2 pb-4" src="papers/figures/arxiv24_lstp.png"> 
                        </div>
                    </td> -->
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <span class="papertitle">Efficient Temporal Extrapolation of Multimodal Large Language Models with Temporal Grounding Bridge</span>
                        <br>
                        <span class="thisauthor">Yuxuan Wang</span>, Yueqian Wang, Pengfei Wu, Jianxin Liang, Dongyan Zhao, Zilong Zheng
                        <br>
                        <span class="conf">
                            <a class="confshort" href="https://2024.emnlp.org/"><b> EMNLP 2024 </b> </a>
                            <span class="tagsep">|</span>
                            <a class="tag" href="https://arxiv.org/abs/2402.16050" target="_blank"><i
                                    class="fa fa-file-pdf-o" aria-hidden="true"></i> PDF</a>
                            <span class="tagsep">|</span>
                            <a class="tag" href="https://github.com/bigai-nlco/LSTP-Chat"
                                target="_blank"><i class="fa fa-github-alt" aria-hidden="true"></i> Code & Demo</a>
                            <span class="tagsep">|</span>
                            <a class="tag" href="papers/arxiv24_lstp.bib" target="_blank"><i class="fa fa-download"
                                    aria-hidden="true"></i> Cite</a>
                        </span>
                    </td>
                </tr>    
                  
                <tr></tr>
                    <!-- <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one" >
                            <img  class="overviewpic pt-2 pb-4" src="papers/figures/aaai24_stair.png"> 
                        </div>
                    </td> -->
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <span class="papertitle">ExoViP: Step-by-step Verification and Exploration with Exoskeleton Modules for Compositional Visual Reasoning</span>
                        <br>
                        <span class="thisauthor">Yuxuan Wang</span>, Alan Yuille, Zhuowan Li, Zilong Zheng
                        <br>
                        <span class="conf">
                            <a class="confshort" href="https://colmweb.org"><b> COLM 2024 </b> </a>
                            <span class="tagsep">|</span>
                            <a class="tag" href="https://arxiv.org/abs/2408.02210" target="_blank"><i
                                    class="fa fa-file-pdf-o" aria-hidden="true"></i> PDF</a>
                            <span class="tagsep">|</span>
                            <a class="tag" href="https://github.com/bigai-nlco/ExoViP"
                                target="_blank"><i class="fa fa-github-alt" aria-hidden="true"></i> Code</a>
                            <span class="tagsep">|</span>
                            <a class="tag" href="papers/colm24_exovip.bib" target="_blank"><i class="fa fa-download"
                                    aria-hidden="true"></i> Cite</a>
                        </span>
                    </td>
                </tr>     
                    

                <tr></tr>
                    <!-- <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one" >
                            <img  class="overviewpic pt-2 pb-4" src="papers/figures/aaai24_stair.png"> 
                        </div>
                    </td> -->
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <span class="papertitle">STAIR: Spatial-Temporal Reasoning with Auditable Intermediate Results for Video Question Answering</span>
                        <br>
                        Yueqian Wang, <span class="thisauthor">Yuxuan Wang</span>, Kai Chen, Dongyan Zhao
                        <br>
                        <span class="conf">
                            <a class="confshort" href="https://aaai.org/aaai-conference/"><b> AAAI 2024 </b> </a>
                            <span class="tagsep">|</span>
                            <a class="tag" href="https://arxiv.org/abs/2401.03901" target="_blank"><i
                                    class="fa fa-file-pdf-o" aria-hidden="true"></i> PDF</a>
                            <span class="tagsep">|</span>
                            <a class="tag" href="https://github.com/yellow-binary-tree/STAIR"
                                target="_blank"><i class="fa fa-github-alt" aria-hidden="true"></i> Code</a>
                            <span class="tagsep">|</span>
                            <a class="tag" href="papers/aaai24_stair.bib" target="_blank"><i class="fa fa-download"
                                    aria-hidden="true"></i> Cite</a>
                        </span>
                    </td>
                </tr>     

                <tr></tr>
                    <!-- <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one" >
                            <img  class="overviewpic pt-2 pb-4" src="papers/figures/acl23_vstar.png"> 
                        </div>
                    </td> -->
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <span class="papertitle">VSTAR: A Video-grounded Dialogue Dataset for Situated Semantic Understanding with Scene and Topic Transitions</span>
                        <br>
                        <span class="thisauthor">Yuxuan Wang</span>, Zilong Zheng, Xueliang Zhao, Jinpeng Li, Yueqian Wang, Dongyan Zhao
                        <br>
                        <span class="conf">
                            <a class="confshort" href="https://2023.aclweb.org/"><b> ACL 2023 </b> </a>
                            <span class="tagsep">|</span>
                            <a class="tag" href="https://arxiv.org/abs/2305.18756" target="_blank"><i
                                    class="fa fa-file-pdf-o" aria-hidden="true"></i> PDF</a>
                            <span class="tagsep">|</span>
                            <a class="tag" href="https://github.com/patrick-tssn/VSTAR"
                                target="_blank"><i class="fa fa-github-alt" aria-hidden="true"></i> Code</a>
                            <span class="tagsep">|</span>
                            <a class="tag" href="https://vstar-benchmark.github.io/" target="_blank"><i class="fa fa-home"
                                    aria-hidden="true"></i> Homepage
                            </a>
                            <span class="tagsep">|</span>
                            <a class="tag" href="papers/acl23_vstar.bib" target="_blank"><i class="fa fa-download"
                                    aria-hidden="true"></i> Cite</a>
                        </span>
                    </td>
                </tr>  


                <tr></tr>
                    <!-- <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one" >
                            <img  class="overviewpic pt-2 pb-4" src="papers/figures/acl23_cdbert.png"> 
                        </div>
                    </td> -->
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <span class="papertitle">Shuo Wen Jie Zi: Rethinking Dictionaries and Glyphs for Chinese Language Pre-training </span>
                        <br>
                        <span class="thisauthor">Yuxuan Wang</span>, Jianghui Wang, Dongyan Zhao, Zilong Zheng
                        <br>
                        <span class="conf">
                            <a class="confshort" href="https://2023.aclweb.org/"><b> ACL 2023 Findings </b> </a>
                            <span class="tagsep">|</span>
                            <a class="tag" href="https://arxiv.org/abs/2305.18760" target="_blank"><i
                                    class="fa fa-file-pdf-o" aria-hidden="true"></i> PDF</a>
                            <span class="tagsep">|</span>
                            <a class="tag" href="https://github.com/patrick-tssn/CDBert"
                                target="_blank"><i class="fa fa-github-alt" aria-hidden="true"></i> Code</a>
                            <span class="tagsep">|</span>
                            <a class="tag" href="papers/acl23_cdbert.bib" target="_blank"><i class="fa fa-download"
                                    aria-hidden="true"></i> Cite</a>
                        </span>
                    </td>
                </tr>  

                <tr></tr>
                    <!-- <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one" >
                            <img  class="overviewpic pt-2 pb-4" src="papers/figures/emnlp22_marlsg.png"> 
                        </div>
                    </td> -->
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <span class="papertitle">Collaborative Reasoning on Multi-Modal Semantic Graphs for Video-Grounded Dialogue Generation</span>
                        <br>
                        Xueliang Zhao*, <span class="thisauthor">Yuxuan Wang*</span>, Chongyang Tao, Chenshuo Wang, Dongyan Zhao
                        <br>
                        <span class="conf">
                            <a class="confshort" href="https://2022.emnlp.org/"><b> EMNLP 2022 Findings </b> </a>
                            <span class="tagsep">|</span>
                            <a class="tag" href="https://arxiv.org/abs/2210.12460" target="_blank"><i
                                    class="fa fa-file-pdf-o" aria-hidden="true"></i> PDF</a>
                            <span class="tagsep">|</span>
                            <a class="tag" href="https://github.com/patrick-tssn/MARL_SG"
                                target="_blank"><i class="fa fa-github-alt" aria-hidden="true"></i> Code</a>
                            <span class="tagsep">|</span>
                            <a class="tag" href="papers/emnlp22_marlsg.bib" target="_blank"><i class="fa fa-download"
                                    aria-hidden="true"></i> Cite</a>
                        </span>
                    </td>
                </tr>  

              </tbody></table> 



              <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <!-- <heading>Publications and Preprints</heading> -->
                  <p class="header pt-4"> <b>Preprints</b> <span class="noter" style="color:#7d7d7d"> (* = equal
                    contribution)</span>
            </p>
                </td>
              </tr>
            </tbody></table>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
            
                <tr></tr>
                <!-- <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one" >
                        <img  class="overviewpic pt-2 pb-4" src="papers/figures/arxiv24_lstp.png"> 
                    </div>
                </td> -->
                <td style="padding:20px;width:100%;vertical-align:middle">
                    <span class="papertitle">VideoLLaMB: Long Video Understanding with Recurrent Memory Bridges</span>
                    <br>
                    <span class="thisauthor">Yuxuan Wang</span>, Cihang Xie, Yang Liu, Zilong Zheng
                    <br>
                    <span class="conf">
                        <!-- <a class="confshort" href="https://2023.aclweb.org/"><b> ACL 2023 </b> </a>
                        <span class="tagsep">|</span> -->
                        <a class="tag" href="https://arxiv.org/abs/2409.01071" target="_blank"><i
                                class="fa fa-file-pdf-o" aria-hidden="true"></i> PDF</a>
                        <span class="tagsep">|</span>
                        <a class="tag" href="https://github.com/bigai-nlco/VideoLLaMB"
                            target="_blank"><i class="fa fa-github-alt" aria-hidden="true"></i> Code </a>
                        <span class="tagsep">|</span>
                        <a class="tag" href="https://videollamb.github.io/"
                            target="_blank"><i class="fa fa-home" aria-hidden="true"></i> Homepage </a>
                        <span class="tagsep">|</span>
                        <a class="tag" href="papers/arxiv24_videollamb.bib" target="_blank"><i class="fa fa-download"
                                aria-hidden="true"></i> Cite</a>
                    </span>
                </td>
            </tr>   



            <tr></tr>
                <!-- <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one" >
                        <img  class="overviewpic pt-2 pb-4" src="papers/figures/arxiv24_lstp.png"> 
                    </div>
                </td> -->
                <td style="padding:20px;width:100%;vertical-align:middle">
                    <span class="papertitle">VideoHallucer: Evaluating Intrinsic and Extrinsic Hallucinations in Large Video-Language Models</span>
                    <br>
                    <span class="thisauthor">Yuxuan Wang</span>, Yueqian Wang, Dongyan Zhao, Cihang Xie, Zilong Zheng
                    <br>
                    <span class="conf">
                        <!-- <a class="confshort" href="https://2023.aclweb.org/"><b> ACL 2023 </b> </a>
                        <span class="tagsep">|</span> -->
                        <a class="tag" href="https://arxiv.org/abs/2406.16338" target="_blank"><i
                                class="fa fa-file-pdf-o" aria-hidden="true"></i> PDF</a>
                        <span class="tagsep">|</span>
                        <a class="tag" href="https://github.com/patrick-tssn/VideoHallucer"
                            target="_blank"><i class="fa fa-github-alt" aria-hidden="true"></i> Code </a>
                        <span class="tagsep">|</span>
                        <a class="tag" href="https://videohallucer.github.io/"
                            target="_blank"><i class="fa fa-home" aria-hidden="true"></i> Homepage </a>
                        <span class="tagsep">|</span>
                        <a class="tag" href="papers/arxiv24_videohallucer.bib" target="_blank"><i class="fa fa-download"
                                aria-hidden="true"></i> Cite</a>
                    </span>
                </td>
            </tr>   
            

             

            <tr></tr>
                <!-- <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one" >
                        <img  class="overviewpic pt-2 pb-4" src="papers/figures/arxiv23_moviepuzzle.png"> 
                    </div>
                </td> -->
                <td style="padding:20px;width:100%;vertical-align:middle">
                    <span class="papertitle">MoviePuzzle: Visual Narrative Reasoning through Multimodal Order Learning</span>
                    <br>
                    Jianghui Wang*, <span class="thisauthor">Yuxuan Wang*</span>, Dongyan Zhao, Zilong Zheng 
                    <br>
                    <span class="conf">
                        <!-- <a class="confshort" href="https://2023.aclweb.org/"><b> ACL 2023 </b> </a>
                        <span class="tagsep">|</span> -->
                        <a class="tag" href="https://arxiv.org/abs/2306.02252" target="_blank"><i
                                class="fa fa-file-pdf-o" aria-hidden="true"></i> PDF</a>
                        <span class="tagsep">|</span>
                        <a class="tag" href="https://github.com/MoviePuzzle/MoviePuzzle"
                            target="_blank"><i class="fa fa-github-alt" aria-hidden="true"></i> Code</a>
                        <span class="tagsep">|</span>
                        <a class="tag" href="https://moviepuzzle.github.io/" target="_blank"><i class="fa fa-home"
                                aria-hidden="true"></i> Homepage
                        </a>
                        <span class="tagsep">|</span>
                        <a class="tag" href="papers/arxiv23_moviepuzzle.bib" target="_blank"><i class="fa fa-download"
                                aria-hidden="true"></i> Cite</a>
                    </span>
                </td>
            </tr>  
            
            <tr></tr>
                <!-- <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one" >
                        <img  class="overviewpic pt-2 pb-4" src="papers/figures/arxiv23_divert.png"> 
                    </div>
                </td> -->
                <td style="padding:20px;width:100%;vertical-align:middle">
                    <span class="papertitle">VideoLLM Knows When to Speak: Enhancing Time-Sensitive Video Comprehension with Video-Text Duet Interaction Format                    </span>
                    <br>
                    Yueqian Wang, Xiaojun Meng, <span class="thisauthor">Yuxuan Wang</span>, Jianxin Liang, Jiansheng Wei, Huishuai Zhang, Dongyan Zhao 
                    <br>
                    <span class="conf">
                        <!-- <a class="confshort" href="https://2023.aclweb.org/"><b> ACL 2023 </b> </a>
                        <span class="tagsep">|</span> -->
                        <a class="tag" href="https://arxiv.org/abs/2411.17991" target="_blank"><i
                                class="fa fa-file-pdf-o" aria-hidden="true"></i> PDF</a>
                        <span class="tagsep">|</span>
                        <a class="tag" href="https://github.com/yellow-binary-tree/MMDuet"
                            target="_blank"><i class="fa fa-github-alt" aria-hidden="true"></i> Code</a>
                        <span class="tagsep">|</span>
                        <a class="tag" href="papers/arxiv24_mmduet.bib" target="_blank"><i class="fa fa-download"
                                aria-hidden="true"></i> Cite</a>
                    </span>
                </td>
            </tr>

            <tr></tr>
                <!-- <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one" >
                        <img  class="overviewpic pt-2 pb-4" src="papers/figures/arxiv23_divert.png"> 
                    </div>
                </td> -->
                <td style="padding:20px;width:100%;vertical-align:middle">
                    <span class="papertitle">Understanding Multimodal Hallucination with Parameter-Free Representation Alignment                    </span>
                    <br>
                    Yueqian Wang, Jianxin Liang, <span class="thisauthor">Yuxuan Wang</span>, Huishuai Zhang, Dongyan Zhao 
                    <br>
                    <span class="conf">
                        <!-- <a class="confshort" href="https://2023.aclweb.org/"><b> ACL 2023 </b> </a>
                        <span class="tagsep">|</span> -->
                        <a class="tag" href="https://arxiv.org/abs/2409.01151" target="_blank"><i
                                class="fa fa-file-pdf-o" aria-hidden="true"></i> PDF</a>
                        <span class="tagsep">|</span>
                        <a class="tag" href="https://github.com/yellow-binary-tree/Pfram"
                            target="_blank"><i class="fa fa-github-alt" aria-hidden="true"></i> Code</a>
                        <span class="tagsep">|</span>
                        <a class="tag" href="papers/arxiv24_pframe.bib" target="_blank"><i class="fa fa-download"
                                aria-hidden="true"></i> Cite</a>
                    </span>
                </td>
            </tr>  

            <tr></tr>
                <!-- <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one" >
                        <img  class="overviewpic pt-2 pb-4" src="papers/figures/arxiv23_divert.png"> 
                    </div>
                </td> -->
                <td style="padding:20px;width:100%;vertical-align:middle">
                    <span class="papertitle">HawkEye: Training Video-Text LLMs for Grounding Text in Videos</span>
                    <br>
                    Yueqian Wang, Xiaojun Meng, Jianxin Liang, <span class="thisauthor">Yuxuan Wang</span>, Qun Liu, Dongyan Zhao 
                    <br>
                    <span class="conf">
                        <!-- <a class="confshort" href="https://2023.aclweb.org/"><b> ACL 2023 </b> </a>
                        <span class="tagsep">|</span> -->
                        <a class="tag" href="https://arxiv.org/abs/2403.10228" target="_blank"><i
                                class="fa fa-file-pdf-o" aria-hidden="true"></i> PDF</a>
                        <span class="tagsep">|</span>
                        <a class="tag" href="https://github.com/yellow-binary-tree/HawkEye"
                            target="_blank"><i class="fa fa-github-alt" aria-hidden="true"></i> Code</a>
                        <span class="tagsep">|</span>
                        <a class="tag" href="papers/arxiv24_hawkeye.bib" target="_blank"><i class="fa fa-download"
                                aria-hidden="true"></i> Cite</a>
                    </span>
                </td>
            </tr>  

            <tr></tr>
                <!-- <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one" >
                        <img  class="overviewpic pt-2 pb-4" src="papers/figures/arxiv23_divert.png"> 
                    </div>
                </td> -->
                <td style="padding:20px;width:100%;vertical-align:middle">
                    <span class="papertitle">Teaching Text-to-Image Models to Communicate</span>
                    <br>
                    Xiaowen Sun*, Jiazhan Feng*, <span class="thisauthor">Yuxuan Wang</span>, Yuxuan Lai, Dongyan Zhao 
                    <br>
                    <span class="conf">
                        <!-- <a class="confshort" href="https://2023.aclweb.org/"><b> ACL 2023 </b> </a>
                        <span class="tagsep">|</span> -->
                        <a class="tag" href="https://arxiv.org/abs/2309.15516" target="_blank"><i
                                class="fa fa-file-pdf-o" aria-hidden="true"></i> PDF</a>
                        <span class="tagsep">|</span>
                        <!-- <a class="tag" href="https://github.com/"
                            target="_blank"><i class="fa fa-github-alt" aria-hidden="true"></i> Code</a>
                        <span class="tagsep">|</span> -->
                        <a class="tag" href="papers/arxiv23_divert.bib" target="_blank"><i class="fa fa-download"
                                aria-hidden="true"></i> Cite</a>
                    </span>
                </td>
            </tr>  


          </tbody></table> 
        

                


                <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr>
                        <td style="padding:20px;width:100%;vertical-align:middle">
                            <p class="header pt-4"> <b>In Submission</b> </p>
                            VideoQA, Multi-modal Translation, Video Dialogue Temporal Learning, LLM+X, Neural-Symbolic, Complex Reasoning

                        </td>
                    </tr>
                </tbody></table> -->

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr>
                        <td style="padding:20px;width:100%;vertical-align:middle">
                            <p class="header pt-4"> <b>Open-Source Projects</b> </p>
                            <a class="proj" href="https://github.com/bigai-nlco/NeedleInAVideoHaystack"><b> Multimodal Needle In A Video Haystack </b> </a>
                            <object data="https://img.shields.io/github/stars/bigai-nlco/NeedleInAVideoHaystack.svg?style=social&label=Star" > </object>
                            <br>
                            Pressure Testing Large Video-Language Models (LVLM): Doing multimodal retrieval from LVLM at various video lengths to measure accuracy.
                            <br>
                            
                            <a class="proj" href="https://github.com/patrick-tssn/Streaming-Grounded-SAM-2"><b> Streaming Grounded SAM 2</b> </a>
                            <object data="https://img.shields.io/github/stars/patrick-tssn/Streaming-Grounded-SAM-2.svg?style=social&label=Star" > </object>
                            <br>
                            Grounded SAM 2 for streaming video tracking using natural language queries.



                        </td>
                    </tr>
                </tbody></table>



                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr>
                        <td style="padding:20px;width:100%;vertical-align:middle">
                            <p class="header pt-4"> <b>Open-Source Learning Hub</b> </p>
                            <a class="proj" href="https://github.com/patrick-tssn/Awesome-Colorful-LLM"><b> Colorful Multimodal Research </b> </a>
                            <object data="https://img.shields.io/github/last-commit/patrick-tssn/Awesome-Colorful-LLM?style=flat" > </object>
                            <object data="https://img.shields.io/github/stars/patrick-tssn/Awesome-Colorful-LLM.svg?style=social&label=Star" > </object>
                            <br>
                            Recent advancements propelled by large language models (LLMs), encompassing an array of domains including Vision, Audio, Agent, Robotics, and Fundamental Sciences such as Mathematics.
                            <br>
                            <a class="proj" href="https://github.com/patrick-tssn/LM-Research-Hub"><b> Language Modeling Research Hub </b> </a>
                            <object data="https://img.shields.io/github/last-commit/patrick-tssn/LM-Research-Hub?style=flat" > </object>
                            <object data="https://img.shields.io/github/stars/patrick-tssn/LLM4Academic.svg?style=social&label=Star" > </object>
                            <br>
                            A comprehensive compendium for enthusiasts and scholars delving into the fascinating realm of language models (LMs), with a particular focus on large language models (LLMs).
                            <br>
                            <a class="proj" href="https://github.com/patrick-tssn/Awesome-Multimodal-Memory"><b> Multimodal Memory Research </b> </a>
                            <object data="https://img.shields.io/github/last-commit/patrick-tssn/Awesome-Multimodal-Memory?style=flat" > </object>
                            <object data="https://img.shields.io/github/stars/patrick-tssn/Awesome-Multimodal-Memory.svg?style=social&label=Star" > </object>
                            <br>
                            Reading List of Memory Augmented Multimodal Research, including multimodal context modeling, memory in vision and robotics, and external memory/knowledge augmented MLLM.
                        </td>
                    </tr>
                </tbody></table>

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr>
                        <td style="padding:20px;width:100%;vertical-align:middle">
                            <p class="header pt-4"> <b>Services</b> </p>
                            <!-- <p class="paper my-2 pl-2"> -->
                                <b>Reviewer: </b> <a>ARR 2023-2024 (Great Review Mention)</a>, <a>CVPR 2024</a>
                                <br>
                                <b>Area Chair: </b> <a>ARR December 2024</a>
                                <br>
                                <b> Organizer: </b> <a href="http://tcci.ccf.org.cn/conference/2022/taskdata.php">NLPCC 2022 Shared Task 4</a>, <a href="http://tcci.ccf.org.cn/conference/2023/taskdata.php">NLPCC 2023 Shared Task 10</a>
                            <!-- </p> -->
                        </td>
                    </tr>
                </tbody></table>

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr>
                        <!-- <td style="padding:20px;width:100%;vertical-align:middle"></td> -->
                        <td style="padding:20px;width:100%;vertical-align:right">
                            <!-- <hr style="margin-top:0px"> -->
                                <!-- <p class="text-right"> Last modified 5/29/2023. </p>                                 -->
                                <!-- <img align="right" src="https://badges.toozhao.com/badges/01GR3KZKWC49EPV79FZTR0PR6C/green.svg" /> -->
                        </td>
                        
                    </tr>
                </tbody></table>


            </td>
        </tr>
    </table>    

    <script crossorigin="anonymous" src="https://cdn.jsdelivr.net/npm/jquery@3.6.1/dist/jquery.min.js"></script>

    <script crossorigin="anonymous" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.min.js"></script>

    <script src="assets/style.js"></script>
</body>
<style>
</style>

</html>