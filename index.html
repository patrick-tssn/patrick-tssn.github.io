<!doctype html>
<html lang="en">

<head>

    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
    <meta content="Master Student, Peking University" name="description">
    <meta content="Yuxuan Wang" name="author">
    <meta content="#222222" name="theme-color">
    <meta content="Yuxuan Wang, dialogue, llm, reasoning, vision language, multimodal, pku, bigai, homepage" name="keywords">

    <!-- <link href="images/nicons/pku_logo.png" rel="icon" sizes="16x16" type="image/png"> -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="assets/style.css" rel="stylesheet">
    <link href="assets/font-awesome.min.css" rel="stylesheet">

    <title>Yuxuan Wang's Homepage</title>
</head>

<body>
    <!-- <h1>Hello, world!</h1> -->
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr style="padding:0px">
            <td style="padding:0px">

                
                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr style="padding:0px">
                    <td style="padding:2.5%;width:63%;vertical-align:middle">
                        <p class="name"><b>Yuxuan Wang</b></p>
                        <p class="email"><code>flagwyx [at] gmail.com</code></p>

                        I am currently a research engineer at the NLP Lab of the <a href="https://www.bigai.ai/">Beijing Institute for General Artificial Intelligence (BIGAI)</a>, led by <a href="https://zilongzheng.github.io/">Zilong Zhen</a> and <a href="https://zhusongchun.net/">Songchun Zhu</a>. 
                        Prior to this, I obtained my Master's degree from <a href="https://www.pku.edu.cn/" >Peking University</a> under the supervision of <a href="https://www.wict.pku.edu.cn/zhaodongyan/en/">Dongyan Zhao</a>. 
                        Additionally, I had the opportunity to be a summer intern at <a href="https://www.jhu.edu/">Johns Hopkins University</a>, where I was mentored by <a href="https://lizw14.github.io/">Zhuowan Li</a> and <a href="https://www.cs.jhu.edu/~ayuille/">Alan L. Yuille</a>. 
                        My current research primarily concentrates on the domains of video-language learning and multimodal agents. I am especially captivated by studies that provide novel insights and helpful applications.
                        
                        <br> <br>

                        <b> I am looking for a PhD position. Please feel free to contact me without any hesitation! </b>
                        

                    </td>
                    <td style="padding:15% 7% 7% 7%;width:40%;max-width:40%">
                        <!-- <img class="profilepic pt-2 pb-4" src="https://cdn.jsdelivr.net/gh/patrick-tssn/patrick-tssn.github.io@v1.0.0/images/profile.png"> -->
                        <img class="profilepic pt-2 pb-4" src="images/profile.jpg">
                        <!-- <a href="images/profile.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/profile.png" class="hoverZoomLink"></a> -->
                        <div>
                            <a class="menulink" href="https://scholar.google.com/citations?user=jNCX2g0AAAAJ&hl=en"
                                target="_blank">[Scholar]</a>  
                            <a class="menulink" href="https://github.com//patrick-tssn" target="_blank">[Github]</a>  
                            <!-- <a class="menulink" href="https://twitter.com/Patrick_TSSN" target="_blank">Twitter</a> / -->
                            <!-- <a class="menulink" href="https://cdn.jsdelivr.net/gh/patrick-tssn/patrick-tssn.github.io@4cdbf6e/images/YuxuanWang.pdf" target="_blank">CV</a> -->
                            <a class="menulink" href="images/YuxuanWang.pdf" target="_blank">[CV]</a>
                        </div>
                    </td>
                    </tr>
                </tbody></table>

                <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr>
                        <td style="padding:20px;width:100%;vertical-align:middle">
                            <p class="header pt-3"> <b>News</b> ðŸ”¥ </p>
                            <li> 2023.05: 1 paper got accepted by ACL 2023. <img src="images/new.gif"/> </li>
                            <li> 2023.05: 1 paper got accepted by ACL 2023 Findings. <img src="images/new.gif"/> </li>
                        </td>
                    </tr>
                </tbody></table> -->

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr>
                        <td style="padding:20px;width:100%;vertical-align:middle">
                            <p class="header pt-4"> <b>Selected Publications (Timeline)</b> <span class="noter" style="color:#7d7d7d"> (* = equal
                                    contribution)</span>
                            </p>
            
                            <div class="py-2">

                                <p class="paper my-2 pl-2">
                                    <span class="papertitle">STAIR: Spatial-Temporal Reasoning with Auditable Intermediate Results for Video Question Answering</span>
                                    <br>
                                    Yueqian Wang, <span class="thisauthor">Yuxuan Wang</span>, Kai Chen, Dongyan Zhao
                                    <br>
                                    <span class="conf">
                                        <a class="confshort" href="https://aaai.org/aaai-conference/"><b> AAAI 2024 </b> </a>
                                        <span class="tagsep">|</span>
                                        <a class="tag" href="https://arxiv.org/abs/2401.03901" target="_blank"><i
                                                class="fa fa-file-pdf-o" aria-hidden="true"></i> PDF</a>
                                        <span class="tagsep">|</span>
                                        <a class="tag" href="https://github.com/yellow-binary-tree/STAIR"
                                            target="_blank"><i class="fa fa-github-alt" aria-hidden="true"></i> Code</a>
                                        <span class="tagsep">|</span>
                                        <a class="tag" href="papers/aaai24_stair.bib" target="_blank"><i class="fa fa-download"
                                                aria-hidden="true"></i> Cite</a>
                                    </span>
                                </p>

                                <p class="paper my-2 pl-2">
                                    <span class="papertitle">VSTAR: A Video-grounded Dialogue Dataset for Situated Semantic Understanding with Scene and Topic Transitions</span>
                                    <br>
                                    <span class="thisauthor">Yuxuan Wang</span>, Zilong Zheng, Xueliang Zhao, Jinpeng Li, Yueqian Wang, Dongyan Zhao
                                    <br>
                                    <span class="conf">
                                        <a class="confshort" href="https://2023.aclweb.org/"><b> ACL 2023 </b> </a>
                                        <span class="tagsep">|</span>
                                        <a class="tag" href="https://arxiv.org/abs/2305.18756" target="_blank"><i
                                                class="fa fa-file-pdf-o" aria-hidden="true"></i> PDF</a>
                                        <span class="tagsep">|</span>
                                        <a class="tag" href="https://github.com/patrick-tssn/VSTAR"
                                            target="_blank"><i class="fa fa-github-alt" aria-hidden="true"></i> Code</a>
                                        <span class="tagsep">|</span>
                                        <a class="tag" href="papers/acl23_vstar.bib" target="_blank"><i class="fa fa-download"
                                                aria-hidden="true"></i> Cite</a>
                                    </span>
                                </p>

                                <p class="paper my-2 pl-2">
                                    <span class="papertitle">Shuo Wen Jie Zi: Rethinking Dictionaries and Glyphs for Chinese Language Pre-training </span>
                                    <br>
                                    <span class="thisauthor">Yuxuan Wang</span>, Jianghui Wang, Dongyan Zhao, Zilong Zheng
                                    <br>
                                    <span class="conf">
                                        <a class="confshort" href="https://2023.aclweb.org/"><b> ACL 2023 Findings </b> </a>
                                        <span class="tagsep">|</span>
                                        <a class="tag" href="https://arxiv.org/abs/2305.18760" target="_blank"><i
                                                class="fa fa-file-pdf-o" aria-hidden="true"></i> PDF</a>
                                        <span class="tagsep">|</span>
                                        <a class="tag" href="https://github.com/patrick-tssn/CDBert"
                                            target="_blank"><i class="fa fa-github-alt" aria-hidden="true"></i> Code</a>
                                        <span class="tagsep">|</span>
                                        <a class="tag" href="papers/acl23_cdbert.bib" target="_blank"><i class="fa fa-download"
                                                aria-hidden="true"></i> Cite</a>
                                    </span>
                                </p>

                                <p class="paper my-2 pl-2">
                                    <span class="papertitle">Collaborative Reasoning on Multi-Modal Semantic Graphs for Video-Grounded Dialogue Generation</span>
                                    <br>
                                    Xueliang Zhao*, <span class="thisauthor">Yuxuan Wang*</span>, Chongyang Tao, Chenshuo Wang, Dongyan Zhao
                                    <br>
                                    <span class="conf">
                                        <a class="confshort" href="https://2022.emnlp.org/"><b> EMNLP 2022 Findings </b> </a>
                                        <span class="tagsep">|</span>
                                        <a class="tag" href="https://arxiv.org/abs/2210.12460" target="_blank"><i
                                                class="fa fa-file-pdf-o" aria-hidden="true"></i> PDF</a>
                                        <span class="tagsep">|</span>
                                        <a class="tag" href="https://github.com/patrick-tssn/MARL_SG"
                                            target="_blank"><i class="fa fa-github-alt" aria-hidden="true"></i> Code</a>
                                        <span class="tagsep">|</span>
                                        <a class="tag" href="papers/emnlp22_marlsg.bib" target="_blank"><i class="fa fa-download"
                                                aria-hidden="true"></i> Cite</a>
                                    </span>
                                </p>
                            </div>
                        </td>
                    </tr>
                </tbody></table>

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr>
                        <td style="padding:20px;width:100%;vertical-align:middle">
                            <p class="header pt-4"> <b>Preprints</b> <span class="noter" style="color:#7d7d7d"> (* = equal
                                    contribution)</span>
                            </p>
            
                            <div class="py-2">

                                <p class="paper my-2 pl-2">
                                    <span class="papertitle">LSTP: Language-guided Spatial-Temporal Prompt Learning for Long-form Video-Text Understanding</span>
                                    <br>
                                    <span class="thisauthor">Yuxuan Wang</span>, Yueqian Wang, Pengfei Wu, Jianxin Liang, Dongyan Zhao, Zilong Zheng
                                    <br>
                                    <span class="conf">
                                        <!-- <a class="confshort" href="https://2023.aclweb.org/"><b> ACL 2023 </b> </a>
                                        <span class="tagsep">|</span> -->
                                        <a class="tag" href="https://arxiv.org/abs/2402.16050" target="_blank"><i
                                                class="fa fa-file-pdf-o" aria-hidden="true"></i> PDF</a>
                                        <span class="tagsep">|</span>
                                        <a class="tag" href="https://github.com/bigai-nlco/LSTP-Chat"
                                            target="_blank"><i class="fa fa-github-alt" aria-hidden="true"></i> Code</a>
                                        <span class="tagsep">|</span>
                                        <a class="tag" href="papers/arxiv24_lstp.bib" target="_blank"><i class="fa fa-download"
                                                aria-hidden="true"></i> Cite</a>
                                    </span>
                                </p>

                                <p class="paper my-2 pl-2">
                                    <span class="papertitle">Teaching Text-to-Image Models to Communicate</span>
                                    <br>
                                    Xiaowen Sun*, Jiazhan Feng*, <span class="thisauthor">Yuxuan Wang</span>, Yuxuan Lai, Dongyan Zhao 
                                    <br>
                                    <span class="conf">
                                        <!-- <a class="confshort" href="https://2023.aclweb.org/"><b> ACL 2023 </b> </a>
                                        <span class="tagsep">|</span> -->
                                        <a class="tag" href="https://arxiv.org/abs/2309.15516" target="_blank"><i
                                                class="fa fa-file-pdf-o" aria-hidden="true"></i> PDF</a>
                                        <span class="tagsep">|</span>
                                        <!-- <a class="tag" href="https://github.com/"
                                            target="_blank"><i class="fa fa-github-alt" aria-hidden="true"></i> Code</a>
                                        <span class="tagsep">|</span> -->
                                        <a class="tag" href="papers/arxiv23_moviepuzzle.bib" target="_blank"><i class="fa fa-download"
                                                aria-hidden="true"></i> Cite</a>
                                    </span>
                                </p>

                                <p class="paper my-2 pl-2">
                                    <span class="papertitle">MoviePuzzle: Visual Narrative Reasoning through Multimodal Order Learning</span>
                                    <br>
                                    Jianghui Wang*, <span class="thisauthor">Yuxuan Wang*</span>, Dongyan Zhao, Zilong Zheng 
                                    <br>
                                    <span class="conf">
                                        <!-- <a class="confshort" href="https://2023.aclweb.org/"><b> ACL 2023 </b> </a>
                                        <span class="tagsep">|</span> -->
                                        <a class="tag" href="https://arxiv.org/abs/2306.02252" target="_blank"><i
                                                class="fa fa-file-pdf-o" aria-hidden="true"></i> PDF</a>
                                        <span class="tagsep">|</span>
                                        <a class="tag" href="https://github.com/MoviePuzzle/MoviePuzzle"
                                            target="_blank"><i class="fa fa-github-alt" aria-hidden="true"></i> Code</a>
                                        <span class="tagsep">|</span>
                                        <a class="tag" href="papers/arxiv23_moviepuzzle.bib" target="_blank"><i class="fa fa-download"
                                                aria-hidden="true"></i> Cite</a>
                                    </span>
                                </p>

                            </div>
                        </td>
                    </tr>
                </tbody></table>


                <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr>
                        <td style="padding:20px;width:100%;vertical-align:middle">
                            <p class="header pt-4"> <b>In Submission</b> </p>
                            VideoQA, Multi-modal Translation, Video Dialogue Temporal Learning, LLM+X, Neural-Symbolic, Complex Reasoning

                        </td>
                    </tr>
                </tbody></table> -->

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr>
                        <td style="padding:20px;width:100%;vertical-align:middle">
                            <p class="header pt-4"> <b>Open-Source Learning Hub</b> </p>
                            <a class="proj" href="https://github.com/patrick-tssn/Awesome-Colorful-LLM"><b> Colorful Multimodal Research </b> </a>
                            <object data="https://img.shields.io/github/stars/patrick-tssn/Awesome-Colorful-LLM.svg?style=social&label=Star" > </object>
                            <br>
                            Recent advancements propelled by large language models (LLMs), encompassing an array of domains including Vision, Audio, Agent, Robotics, and Fundamental Sciences such as Mathematics.
                            <br>
                            <a class="proj" href="https://github.com/patrick-tssn/LM-Research-Hub"><b> Language Modeling Research Hub </b> </a>
                            <object data="https://img.shields.io/github/stars/patrick-tssn/LLM4Academic.svg?style=social&label=Star" > </object>
                            <br>
                            A comprehensive compendium for enthusiasts and scholars delving into the fascinating realm of language models (LMs), with a particular focus on large language models (LLMs)
                        </td>
                    </tr>
                </tbody></table>

                <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr>
                        <td style="padding:20px;width:100%;vertical-align:middle">
                            <p class="header pt-4"> <b>Experiences</b> </p>
                            <p class="paper my-2 pl-2">
                                <b>Reviewer:</b> EMNLP-22.
                            </p>
                        </td>
                    </tr>
                </tbody></table> -->

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr>
                        <!-- <td style="padding:20px;width:100%;vertical-align:middle"></td> -->
                        <td style="padding:20px;width:100%;vertical-align:right">
                            <hr style="margin-top:0px">
                                <!-- <p class="text-right"> Last modified 5/29/2023. </p>                                 -->
                                <!-- <img align="right" src="https://badges.toozhao.com/badges/01GR3KZKWC49EPV79FZTR0PR6C/green.svg" /> -->
                        </td>
                        
                    </tr>
                </tbody></table>


            </td>
        </tr>
    </table>    

    <script crossorigin="anonymous" src="https://cdn.jsdelivr.net/npm/jquery@3.6.1/dist/jquery.min.js"></script>

    <script crossorigin="anonymous" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.min.js"></script>

    <script src="assets/style.js"></script>
</body>
<style>
</style>

</html>